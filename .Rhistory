# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
SAMPLED_Z = c(SAMPLED_Z, Z_i)
}
}
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
beta_sampled = rdirichlet(1, alpha = c(1,1,1))
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:1000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = c(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
SAMPLED_Z = c(SAMPLED_Z, Z_i)
}
}
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
beta_sampled = rdirichlet(1, alpha = c(1,1,1))
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:1000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = c(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = c(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA, MARGIN = 2, mean)
BETA
# Data generating process
set.seed(0)
n = 300
beta = c(1,3,5)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X0
X0 = rep(1, n)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5))
X2 = as.factor(apply(c(1,2,3)*X2, MARGIN = 2, FUN = sum))
# X
X = cbind(X0, X1, X2)
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
beta_sampled = rdirichlet(1, alpha = c(1,1,1))
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:1000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA, MARGIN = 2, mean)
df_beta= rbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
sampling_beta
# Check posterior expectation of beta
sampling_beta = apply(BETA, MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
# Check posterior expectation of beta
sampling_beta = apply(BETA[501:1000], MARGIN = 2, mean)
# Check posterior expectation of beta
sampling_beta = apply(BETA[501:1000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
beta
View(BETA)
# Data generating process
set.seed(0)
n = 300
beta = c(10,5,8)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X0
X0 = rep(1, n)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5))
X2 = as.factor(apply(c(1,2,3)*X2, MARGIN = 2, FUN = sum))
# X
X = cbind(X0, X1, X2)
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
beta_sampled = rdirichlet(1, alpha = c(1,1,1))
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:1000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA[501:1000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
# Data generating process
set.seed(0)
n = 300
beta = c(5,8)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5))
X2 = as.factor(apply(c(1,2,3)*X2, MARGIN = 2, FUN = sum))
# X
X = cbind(X1, X2)
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:1000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA[501:1000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
View(X)
solve(t(X)%*%X)
dim(solve(t(X)%*%X)%*%t(X))
mean_beta_hat%*%Z_i
max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
mean_beta
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
# Convergence checking Feature 1
matplot(1:dim(BETA[501:1000,])[1], BETA[501:1000,],
type = 'l', ylab = 'Prob', xlab = 'trials',
main = 'Checking stability of marginal pmf of feature 3', ylim = c(0,1))
# Convergence checking Feature 1
matplot(1:dim(BETA[501:1000,])[1], BETA[501:1000,],
type = 'l', ylab = 'Prob', xlab = 'trials',
main = 'Checking stability of marginal pmf of feature 3')
# Data generating process
set.seed(0)
n = 300
beta = c(10,5,8)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X0
X0 = rep(1, n)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5))
X2 = as.factor(apply(c(1,2,3)*X2, MARGIN = 2, FUN = sum))
# X
X = cbind(X0, X1, X2)
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:10000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA[501:1000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
# Check posterior expectation of beta
sampling_beta = apply(BETA[5001:10000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
# Convergence checking Feature 1
matplot(1:dim(BETA[5001:10000,])[1], BETA[501:1000,],
type = 'l', ylab = 'Prob', xlab = 'trials',
main = 'Checking stability of marginal pmf of feature 3')
# Convergence checking Feature 1
matplot(1:dim(BETA[5001:10000,])[1], BETA[5001:10000,],
type = 'l', ylab = 'Prob', xlab = 'trials',
main = 'Checking stability of marginal pmf of feature 3')
rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5))
# Data generating process
set.seed(0)
n = 300
beta = c(3, 5, 1, 4, 7)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X0
X0 = rep(1, n)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = t(rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5)))
# X
X = cbind(X0, X1, X2)
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
View(X)
# Data generating process
set.seed(0)
n = 300
beta = c(5, 1, 4, 7)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = t(rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5)))
# X
X = cbind(X1, X2)
colnames(X) <- c('X1', 'X2_cat1', 'X2_cat2', 'X2_cat3')
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Data generating process
set.seed(0)
n = 300
beta = c(5, 1, 4, 7)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X1
X1 = rnorm(n, mean = 2, sd = 3)
# X2
X2 = t(rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5)))
# X
X = cbind(X1, X2)
colnames(X) <- c('X1', 'X2_cat1', 'X2_cat2', 'X2_cat3')
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:10000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
# Check posterior expectation of beta
sampling_beta = apply(BETA[5001:10000,], MARGIN = 2, mean)
df_beta= cbind(beta, sampling_beta)
colnames(df_beta)<- c('original beta', 'sampled beta')
barplot(df_beta, beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Beta')
# Convergence checking Feature 1
matplot(1:dim(BETA[5001:10000,])[1], BETA[5001:10000,],
type = 'l', ylab = 'Prob', xlab = 'trials',
main = 'Checking stability of marginal pmf of feature 3')
View(Z)
hist(Z)
# Data generating process
set.seed(0)
n = 300
beta = c(5, 1, 4, 7)
# noise term
epsilon = rnorm(n, mean = 0, sd = 1)
# X1
#X1 = rnorm(n, mean = 2, sd = 3)
X1 = runif(n, 2, 10)
# X2
X2 = t(rmultinom(n, size = 1, prob = c(0.2, 0.3, 0.5)))
# X
X = cbind(X1, X2)
colnames(X) <- c('X1', 'X2_cat1', 'X2_cat2', 'X2_cat3')
# Z
Z = X%*%beta + epsilon
# Cut-off points and Y
g = quantile(Z, probs = c(0.2, 0.4, 0.6, 0.8))
Y = rep(NA, n)
Y[Z<g[1]] = 1
Y[Z>=g[1] & Z<g[2]] = 2
Y[Z>=g[2] & Z<g[3]] = 3
Y[Z>=g[3] & Z<g[4]] = 4
Y[Z>=g[4]] = 5
Z
hist(Z)
# Blocked Gibbs Sampling
set.seed(1)
# Prior Parameter
Z_i = Y - 0.5
variance_beta = (n/(n+1))*solve(t(X)%*%X)
mean_beta_hat = (n/(n+1))*solve(t(X)%*%X)%*%t(X)
# Initialize the sampling matrix
SAMPLED_Z = c()
BETA = c()
for (round in 1:10000) {
# Step 1: Sample Beta
mean_beta = mean_beta_hat%*%Z_i
beta_sampled = dae::rmvnorm(mean = mean_beta, V = variance_beta, method = 'choleski')
BETA = rbind(BETA, beta_sampled)
# Step 2: Sample Z using inverse cdf appraoch
for (i in 1:n) {
# Get the lower and upper bound (a, b) of truncated normal
a = max(-Inf, Z_i[Y<Y[i]], na.rm = TRUE)
b = min(Inf, Z_i[Y>Y[i]], na.rm = TRUE)
# Sample using inverse cdf
ez = t(beta_sampled)%*%X[i,]
u = runif(1, pnorm(a - ez), pnorm(b-ez))
Z_i[i] = ez + qnorm(u)
}
SAMPLED_Z = rbind(SAMPLED_Z, Z_i)
}
