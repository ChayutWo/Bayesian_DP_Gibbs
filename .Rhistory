}
# Sample theta using full conditional distribution
theta_1[k,] = rdirichlet(1, a_theta + nk1)
theta_2[k,] = rdirichlet(1, a_theta + nk2)
theta_3[k,] = rdirichlet(1, a_theta + nk3)
}
# Step 4: Sampling alpha
alpha = rgamma(1, shape = a_alp + cluster_num - 1, b_alp - log(pi[cluster_num]))
# Step 5: Sample x3 for missing entries
for (ind in 1:dim(x3)[2]) {
if (indicator[ind]) {
# This entry of x3 is missing
x3[, ind] = rmultinom(1,1,prob = theta_3[z[,3]==1,])
}
}
# Record pmf
sample_pmf1 = rbind(sample_pmf1, apply(as.vector(pi)*theta_1, MARGIN = 2, sum))
sample_pmf2 = rbind(sample_pmf2, apply(as.vector(pi)*theta_2, MARGIN = 2, sum))
sample_pmf3 = rbind(sample_pmf3, apply(as.vector(pi)*theta_3, MARGIN = 2, sum))
}
# Model checking with empirical data: use first 2000 runs as burn-in
# Feature 1
theoretical_pmf1 = apply(as.vector(pi_true)*theta_p1_true,MARGIN = 2, sum)
empirical_pmf1 = apply(x1, MARGIN = 1, mean)
sampling_pmf1 = apply(sample_pmf1[2001:10000,], MARGIN = 2, mean)
df1= rbind(theoretical_pmf1, empirical_pmf1, sampling_pmf1)
colnames(df1)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df1, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 1')
# Feature 2
theoretical_pmf2 = apply(as.vector(pi_true)*theta_p2_true,MARGIN = 2, sum)
empirical_pmf2 = apply(x2, MARGIN = 1, mean)
sampling_pmf2 = apply(sample_pmf2[2001:10000,], MARGIN = 2, mean)
df2 = rbind(theoretical_pmf2, empirical_pmf2, sampling_pmf2)
colnames(df2)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df2, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 2')
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
col3 = as.factor(apply(c(1,2,3)*x3_original, MARGIN = 2, FUN = sum))
MAR_missing3 = c(mean(col3[!indicator]==1), mean(col3[!indicator]==2),
mean(col3[!indicator]==3))
sampling_pmf3 = apply(sample_pmf3[2001:10000,], MARGIN = 2, mean)
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, sampling_pmf3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
matplot(1:dim(sample_pmf1)[1], sample_pi, type = 'l',
ylab = 'Prob of each clusters', xlab = 'trials',
main = 'Label Switching?', ylim = c(0,0.8))
plot(1:length(sample_cluster_used), sample_cluster_used, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time')
# DO NOT ALTER CODE IN THIS CHUNK
# The very first time you use this R markdown file, you should install each of the packages below.
# The same goes for other packages you might decide to use
# Remember that you only need to install each R package once in R (ever -- unless you change computers).
# All you need to do whenever you need to use the package again (after restarting the R session),
# is to use the library function to call the package.
# For example, type install.packages("knitr") in the console to install the knitr package.
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(NPBayesImputeCat)
# Define mixing proportion
set.seed(0)
pi_true = c(0.3, 0.1, 0.6)
# Theta 1 for mixture cluster 1
theta_11_true = c(0.7,0.2,0.1)
theta_12_true = c(0.1,0.8,0.1)
theta_13_true =c(0.2,0.1,0.7)
# Theta 2 for mixture cluster 2
theta_21_true = c(0.05,0.75,0.2)
theta_22_true = c(0.2,0.15,0.65)
theta_23_true =c(0.7,0.2,0.1)
# Theta 3 for mixture cluster 3
theta_31_true = c(0.1,0.1,0.8)
theta_32_true = c(0.7,0.15,0.15)
theta_33_true =c(0.1,0.7,0.2)
# Theta row i is cluster i, column j is category j
theta_p1_true = rbind(theta_11_true, theta_21_true, theta_31_true)
theta_p2_true = rbind(theta_12_true, theta_22_true, theta_32_true)
theta_p3_true = rbind(theta_13_true, theta_23_true, theta_33_true)
# Create simulated data
n = 300
class_i = rmultinom(n, size = 1, prob = pi_true)
x1 = c()
x2 = c()
x3_original = c()
for (i in 1:n) {
x1 = cbind(x1, rmultinom(1, size = 1, prob = theta_p1_true[class_i[, i]==1,]))
x2 = cbind(x2, rmultinom(1, size = 1, prob = theta_p2_true[class_i[, i]==1,]))
x3_original = cbind(x3_original, rmultinom(1, size = 1, prob = theta_p3_true[class_i[, i]==1,]))
}
# Format data input
col1 = as.factor(apply(c(1,2,3)*x1, MARGIN = 2, FUN = sum))
col2 = as.factor(apply(c(1,2,3)*x2, MARGIN = 2, FUN = sum))
col3 = as.factor(apply(c(1,2,3)*x3_original, MARGIN = 2, FUN = sum))
df = data.frame(col1, col2, col3)
colnames(df)<- c('feature1', 'feature2', 'feature3')
# Define parameter of logistic function
w1 =  c(-0.75, -1, 0.2)
w2 = c(0.6, -2, 0.5)
w3 = c(0.2, -0.8, -0.4)
# Calculate probability of missingness of features 3
prob = apply(w1*x1, MARGIN = 2, FUN = sum) +
apply(w2*x2, MARGIN = 2, FUN = sum) +
apply(w3*x3_original, MARGIN = 2, FUN = sum)
prob = 1/(exp(-prob)+1)
# Indicator for X3miss
indicator = rbernoulli(n = 300, p = prob)
x3 = x3_original
x3[, indicator] = c(NA, NA, NA)
iter = 10000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(1000,iter,1)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
imputation_3 = c(mean(ImputedX$feature3==1), mean(ImputedX$feature3==2),
mean(ImputedX$feature3==3))
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
dim(imputed_df)
iter = 8000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(2000,iter,1)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
imputation_3 = c(mean(ImputedX$feature3==1), mean(ImputedX$feature3==2),
mean(ImputedX$feature3==3))
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
dim(imputed_df)
View(imputed_df)
table(imputed_df[,601:900])
View(x3)
View(df)
# Define mixing proportion
set.seed(0)
pi_true = c(0.3, 0.1, 0.6)
# Theta 1 for mixture cluster 1
theta_11_true = c(0.7,0.2,0.1)
theta_12_true = c(0.1,0.8,0.1)
theta_13_true =c(0.2,0.1,0.7)
# Theta 2 for mixture cluster 2
theta_21_true = c(0.05,0.75,0.2)
theta_22_true = c(0.2,0.15,0.65)
theta_23_true =c(0.7,0.2,0.1)
# Theta 3 for mixture cluster 3
theta_31_true = c(0.1,0.1,0.8)
theta_32_true = c(0.7,0.15,0.15)
theta_33_true =c(0.1,0.7,0.2)
# Theta row i is cluster i, column j is category j
theta_p1_true = rbind(theta_11_true, theta_21_true, theta_31_true)
theta_p2_true = rbind(theta_12_true, theta_22_true, theta_32_true)
theta_p3_true = rbind(theta_13_true, theta_23_true, theta_33_true)
# Create simulated data
n = 300
class_i = rmultinom(n, size = 1, prob = pi_true)
x1 = c()
x2 = c()
x3_original = c()
for (i in 1:n) {
x1 = cbind(x1, rmultinom(1, size = 1, prob = theta_p1_true[class_i[, i]==1,]))
x2 = cbind(x2, rmultinom(1, size = 1, prob = theta_p2_true[class_i[, i]==1,]))
x3_original = cbind(x3_original, rmultinom(1, size = 1, prob = theta_p3_true[class_i[, i]==1,]))
}
# Format data input
col1 = as.factor(apply(c(1,2,3)*x1, MARGIN = 2, FUN = sum))
col2 = as.factor(apply(c(1,2,3)*x2, MARGIN = 2, FUN = sum))
col3 = as.factor(apply(c(1,2,3)*x3_original, MARGIN = 2, FUN = sum))
df = data.frame(col1, col2, col3)
colnames(df)<- c('feature1', 'feature2', 'feature3')
# Define parameter of logistic function
w1 =  c(-0.75, -1, 0.2)
w2 = c(0.6, -2, 0.5)
w3 = c(0.2, -0.8, -0.4)
# Calculate probability of missingness of features 3
prob = apply(w1*x1, MARGIN = 2, FUN = sum) +
apply(w2*x2, MARGIN = 2, FUN = sum) +
apply(w3*x3_original, MARGIN = 2, FUN = sum)
prob = 1/(exp(-prob)+1)
# Indicator for X3miss
indicator = rbernoulli(n = 300, p = prob)
df[indicator, 3] = c(NA)
iter = 8000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(2000,iter,1)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
imputation_3 = c(mean(ImputedX$feature3==1), mean(ImputedX$feature3==2),
mean(ImputedX$feature3==3))
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
View(imputed_df)
View(df)
[seq(1, dim(imputed_df)[2], 3)]
seq(1, dim(imputed_df)[2], 3)
seq(3, dim(imputed_df)[2], 3)
table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
mean(table(imputed_df[,seq(3, dim(imputed_df)[2], 3)]))
feature3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
feature3 = feature3/sum(feature3)
feature3
feature3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
feature3
# Model checking with empirical data
# Extract only features 3 from imputed data
feature3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
feature3 = feature3/sum(feature3)
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
imputation_3 = feature3
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
table(ImputedX)
table(ImputedX[,3])
table(ImputedX[,3])/sum(table(ImputedX[,3]))
# Define mixing proportion
set.seed(0)
pi_true = c(0.3, 0.1, 0.6)
# Theta 1 for mixture cluster 1
theta_11_true = c(0.7,0.2,0.1)
theta_12_true = c(0.1,0.8,0.1)
theta_13_true =c(0.2,0.1,0.7)
# Theta 2 for mixture cluster 2
theta_21_true = c(0.05,0.75,0.2)
theta_22_true = c(0.2,0.15,0.65)
theta_23_true =c(0.7,0.2,0.1)
# Theta 3 for mixture cluster 3
theta_31_true = c(0.1,0.1,0.8)
theta_32_true = c(0.7,0.15,0.15)
theta_33_true =c(0.1,0.7,0.2)
# Theta row i is cluster i, column j is category j
theta_p1_true = rbind(theta_11_true, theta_21_true, theta_31_true)
theta_p2_true = rbind(theta_12_true, theta_22_true, theta_32_true)
theta_p3_true = rbind(theta_13_true, theta_23_true, theta_33_true)
# Create simulated data
n = 300
class_i = rmultinom(n, size = 1, prob = pi_true)
x1 = c()
x2 = c()
x3_original = c()
for (i in 1:n) {
x1 = cbind(x1, rmultinom(1, size = 1, prob = theta_p1_true[class_i[, i]==1,]))
x2 = cbind(x2, rmultinom(1, size = 1, prob = theta_p2_true[class_i[, i]==1,]))
x3_original = cbind(x3_original, rmultinom(1, size = 1, prob = theta_p3_true[class_i[, i]==1,]))
}
# Format data input
col1 = as.factor(apply(c(1,2,3)*x1, MARGIN = 2, FUN = sum))
col2 = as.factor(apply(c(1,2,3)*x2, MARGIN = 2, FUN = sum))
col3 = as.factor(apply(c(1,2,3)*x3_original, MARGIN = 2, FUN = sum))
df = data.frame(col1, col2, col3)
colnames(df)<- c('feature1', 'feature2', 'feature3')
# Define parameter of logistic function
w1 =  c(-0.75, -1, 0.2)
w2 = c(0.6, -2, 0.5)
w3 = c(0.2, -0.8, -0.4)
# Calculate probability of missingness of features 3
prob = apply(w1*x1, MARGIN = 2, FUN = sum) +
apply(w2*x2, MARGIN = 2, FUN = sum) +
apply(w3*x3_original, MARGIN = 2, FUN = sum)
prob = 1/(exp(-prob)+1)
# Indicator for X3miss
indicator = rbernoulli(n = 300, p = prob)
df[indicator, 3] = c(NA)
iter = 8000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(2000,iter,1)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
# Extract only features 3 from imputed data
imputation_3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
imputation_3 = imputation_3/sum(imputation_3)
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
iter = 8000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(2000,iter,5)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
# Extract only features 3 from imputed data
imputation_3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
imputation_3 = imputation_3/sum(imputation_3)
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
# Define mixing proportion
set.seed(0)
pi_true = c(0.3, 0.1, 0.6)
# Theta 1 for mixture cluster 1
theta_11_true = c(0.7,0.2,0.1)
theta_12_true = c(0.1,0.8,0.1)
theta_13_true =c(0.2,0.1,0.7)
# Theta 2 for mixture cluster 2
theta_21_true = c(0.05,0.75,0.2)
theta_22_true = c(0.2,0.15,0.65)
theta_23_true =c(0.7,0.2,0.1)
# Theta 3 for mixture cluster 3
theta_31_true = c(0.1,0.1,0.8)
theta_32_true = c(0.7,0.15,0.15)
theta_33_true =c(0.1,0.7,0.2)
# Theta row i is cluster i, column j is category j
theta_p1_true = rbind(theta_11_true, theta_21_true, theta_31_true)
theta_p2_true = rbind(theta_12_true, theta_22_true, theta_32_true)
theta_p3_true = rbind(theta_13_true, theta_23_true, theta_33_true)
# Create simulated data
n = 300
class_i = rmultinom(n, size = 1, prob = pi_true)
x1 = c()
x2 = c()
x3_original = c()
for (i in 1:n) {
x1 = cbind(x1, rmultinom(1, size = 1, prob = theta_p1_true[class_i[, i]==1,]))
x2 = cbind(x2, rmultinom(1, size = 1, prob = theta_p2_true[class_i[, i]==1,]))
x3_original = cbind(x3_original, rmultinom(1, size = 1, prob = theta_p3_true[class_i[, i]==1,]))
}
# Format data input
col1 = as.factor(apply(c(1,2,3)*x1, MARGIN = 2, FUN = sum))
col2 = as.factor(apply(c(1,2,3)*x2, MARGIN = 2, FUN = sum))
col3 = as.factor(apply(c(1,2,3)*x3_original, MARGIN = 2, FUN = sum))
df = data.frame(col1, col2, col3)
colnames(df)<- c('feature1', 'feature2', 'feature3')
# Define parameter of logistic function
w1 =  c(-0.75, -1, 0.2)
w2 = c(0.6, -2, 0.5)
w3 = c(0.2, -0.8, -0.4)
# Calculate probability of missingness of features 3
prob = apply(w1*x1, MARGIN = 2, FUN = sum) +
apply(w2*x2, MARGIN = 2, FUN = sum) +
apply(w3*x3_original, MARGIN = 2, FUN = sum)
prob = 1/(exp(-prob)+1)
# Indicator for X3miss
indicator = rbernoulli(n = 300, p = prob)
df[indicator, 3] = c(NA)
iter = 10000
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = df, MCZ = NULL, K = 30, Nmax = 0,
aalpha = 0.25, balpha = 0.25, seed = 1)
# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX'),iter)
# 3. Run model
model$Run(5000,iter,1)
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
imputed_df <- output$ImputedX
#retrieve parameters from the final iteration
result <- model$snapshot
#convert ImputedX matrix to dataframe, using proper factors/names etc.
ImputedX <- GetDataFrame(result$ImputedX,df)
# Model checking with empirical data
# Feature 3
theoretical_pmf3 = apply(as.vector(pi_true)*theta_p3_true,MARGIN = 2, sum)
empirical_pmf3 = apply(x3_original, MARGIN = 1, mean)
MAR_missing3 = c(mean(df[!indicator,3]==1), mean(df[!indicator,3]==2),
mean(df[!indicator,3]==3))
# Extract only features 3 from imputed data
imputation_3 = table(imputed_df[,seq(3, dim(imputed_df)[2], 3)])
imputation_3 = imputation_3/sum(imputation_3)
df3 = rbind(theoretical_pmf3,empirical_pmf3, MAR_missing3, imputation_3)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3')
barplot(df3, xlab = 'Category', beside = TRUE,
legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Feature 3')
plot(1:length(k_star), k_star, xlab = 'trials',
ylab = 'number of active clusters',
main = 'Number of clusters used over time', ylim = c(0,30))
